training_config = dict(
    logging_steps=100,
    train_batch_size=16,
    eval_batch_size=16,
    eval_accumulation_steps=None,
    gradient_accumulation_steps=1,
    epochs=50,
    learning_rate=1e-4,
    weight_decay=0.01,
    adam_beta1=0.9,
    adam_beta2=0.999,
    adam_epsilon=1e-8,
    lr_scheduler_type="linear",
    lr_scheduler_kwargs={},
    collator_type="audio",
    warmup_steps=500,
    save_steps=1000,
    eval_steps=500,
    output_dir="./outputs",
    save_total_limit=3,
    load_best_model_at_end=True,
    metric_for_best_model="eval_f1",
    greater_is_better=True,
    early_stopping_patience=5,
    early_stopping_threshold=0.01
)