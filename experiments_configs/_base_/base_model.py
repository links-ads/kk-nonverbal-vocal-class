model_config = dict(
    max_duration=7,
    adapter_hidden_dim=64,
    embedding_prompt_dim=10,
    lora_rank=8,
    use_weighted_layer_sum=True,
    classifier_proj_size=256,
)